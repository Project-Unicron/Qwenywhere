Hey cutie pies.
It’s me, your bud, Lynn Cole.

Thanks for considering Qwenywhere, it’s a really cool app. I had fun writing it. 

So basically what you’re looking at is a dog simple api server that’s easy to set up and run, on just about any modern computer. It scans your hardware to figure out what the most powerful model you can run is, then it self configures, downloads the model, and before you know it, you’re ready to start writing api calls!

I wrote it to solve one core problem. Writing ai apps is fun, but, writing ai apps also means huge spends on tokens, or turns, or whatever it is the api providers decide they want to charge you for. 

The reason we usually use api providers is because setting up your own inference pipeline is an absolute bitch. It’s complicated, error prone, and usually involves picking from long lists of gpu’s to rent, and linux configuration and this, that, and the other. It’s a lot. And even when you’re using AI to do it, it can still be challenging.

My thinking here is that I wanted one that didn’t necessarily require a high end gpu, but could still use one if it’s on board. I wanted something fast, lightweight, and configurable, but also have all the tools you need to develop with your agentic sandboxes. 

Obviously, it’s designed for local builds, and as far as I know, it’s not multi-modal. 

But if you’re developing fun little agent apps, and you need some easy lightweight inference to play with, it might be just the thing.

Love ya,
–Lynn
